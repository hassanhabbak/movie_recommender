# AUTOGENERATED! DO NOT EDIT! File to edit: 01_users.ipynb (unless otherwise specified).

__all__ = ['add_labels', 'create_training_data', 'train_nn_user_behaviour', 'extract_weights', 'eval_model',
           'get_user_movie_output', 'get_movie_to_movie_rating']

# Cell
import numpy as np
import pandas as pd
from IPython.display import HTML, Image
import os
from sklearn import preprocessing, dummy, metrics, ensemble
from sklearn.model_selection import train_test_split

import keras.models as kmodels
import keras.layers as klayers
import keras.backend as K
import keras

import umap
import matplotlib.pyplot as plt

import plotly.express as px

import tensorflow as tf

# Cell
def add_labels(df):
    """Adds labels to user and movie ids to be sequential"""
    df = df.copy()

    user_le = preprocessing.LabelEncoder()
    user_labels = user_le.fit_transform(df['userId'])

    df['user_label'] = user_labels

    movie_le = preprocessing.LabelEncoder()
    movie_labels = movie_le.fit_transform(df['movieId'])

    df['movie_label'] = movie_labels

    return df, user_le, movie_le

# Cell
def create_training_data(movieid, userid, rating):

    # Create train and validate data
    movie_train, movie_val, user_train, user_val, rating_train, rating_val = train_test_split(movieid, userid, rating, test_size=0.2)

    # Second split to get test sets: 0.25 * 0.8 = 0.2
    movie_train, movie_test, user_train, user_test, rating_train, rating_test = train_test_split(movie_train, user_train, rating_train, test_size=0.25)

    return movie_train, movie_val, movie_test, user_train, user_val, \
            user_test, rating_train, rating_val, rating_test

# Cell
def train_nn_user_behaviour(df, movie_train, movie_val, user_train, user_val, rating_train, rating_val):
    """Trains the regression model and returns it"""

    config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 6} )
    sess = tf.Session(config=config)
    K.set_session(sess)

    # Count the movies and users
    n_movies = df.movie_label.max()
    n_users = df.user_label.max()

    # Now, the deep learning regression

    # First, we take the movie and vectorize it.
    movie_input = keras.layers.Input(shape=[1])
    movie_vec = keras.layers.Flatten()(keras.layers.Embedding(n_movies + 1, 32, name = 'movie_vec')(movie_input))
    movie_vec = keras.layers.Dropout(0.5)(movie_vec)

    # Vectorize users
    user_input = keras.layers.Input(shape=[1])
    user_vec = keras.layers.Flatten()(keras.layers.Embedding(n_users + 1, 32, name = 'user_vec')(user_input))
    user_vec = keras.layers.Dropout(0.5)(user_vec)

    # Build the layers
    input_vecs = keras.layers.concatenate([movie_vec, user_vec])
    nn = keras.layers.Dropout(0.5)(keras.layers.Dense(128, activation='relu')(input_vecs))
    nn = keras.layers.normalization.BatchNormalization()(nn)
    nn = keras.layers.Dropout(0.5)(keras.layers.Dense(128, activation='relu')(nn))
    nn = keras.layers.normalization.BatchNormalization()(nn)
    nn = keras.layers.Dense(128, activation='relu')(nn)

    # The regression output
    result = keras.layers.Dense(1, activation='linear')(nn)

    # And make a model from it that we can actually run.
    model = kmodels.Model([movie_input, user_input], result)
    model.compile('adam', 'mse')

    # These models are to be used if individual vectors of movies need to be computed
    final_layer = kmodels.Model([movie_input, user_input], nn)
    movie_vec = kmodels.Model(movie_input, movie_vec)

    try:
        with tf.device("gpu:0"):
            print("tf.keras code in this scope will run on GPU")
            history = model.fit([movie_train, user_train], rating_train,
                                validation_data=([movie_val, user_val], rating_val),
                                 epochs=100, batch_size = 10000)

    except KeyboardInterrupt:
        pass

    return model, history

# Cell
def extract_weights(name, model):
    """Extract weights from a neural network model"""

    # Extract weights
    weight_layer = model.get_layer(name)
    weights = weight_layer.get_weights()[0]

    # Normalize
    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))
    return weights

# Cell
def eval_model(model, data, y_true ):
    y_pred = model.predict([movie_test, user_test])
    return metrics.mean_squared_error(y_true, y_pred, squared=False)

# Cell
def get_user_movie_output(model, eval_df, user_le, movie_le):
    df = eval_df.copy()

    df['rating'] = model.predict([movie_le.transform(eval_df['movieId']),
                                      user_le.transform(eval_df['userId'])])

    # Clip output to desired ratings
    df['rating'] = df['rating'].clip(upper=5.0, lower=0.0)
    return df

# Cell
def get_movie_to_movie_rating(model, movie_le, embedding_df):


    embedding = extract_weights('movie_vec', model)
    movie_embedding_df = pd.DataFrame.from_records(embedding)
    movie_embedding_df['movie_label'] = movie_le.transform(movie_le.classes_)
    movie_embedding_df['movieId'] = movie_le.classes_
    movie_embedding_df.head()


    d = movie_embedding_df.merge(embedding_df.reset_index(), left_on='movieId', right_on='id', how='inner')
    d

    reducer = umap.UMAP(n_components=10)
    movie_vec_content = reducer.fit_transform(preprocessing.MinMaxScaler().fit_transform(d.drop(columns=['movie_label',
                                                                                                     'movieId', 'id'])))

    movie_similarities = metrics.pairwise.cosine_similarity(movie_vec_content)

    movie_id_dict = d[['movie_label', 'movieId']].drop_duplicates().reset_index(drop=True).sort_values('movie_label').set_index('movie_label').to_dict()['movieId']

    ids = d['movieId'].values
    results = []
    for i in range(movie_similarities.shape[0]):
        i_id = ids[i]
        for j in range(i+1, movie_similarities.shape[1]):
            results.append((i_id, ids[j], movie_similarities[i][j]))

    return pd.DataFrame.from_records( results, columns=['movieId', 'movieId', 'rating'])